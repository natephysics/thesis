% !TEX root = ../my-thesis.tex
%
\chapter{Model Design}
\label{sec:code:model}

Machine learning (ML) models are algorithms that use data and statistical operations to optimize the weights that compose the model.
There are two broad catagories of ML models, supervised and unsupervised.
Supervised models are ones which use labeled data, or data which the expected output of the model is known in advance.
Unsupervised models are models for which a pattern is learned from the data directly.
The model use for this project is a supervised ML model because we know what the expected inputs and outputs should be and can produce labels for both.

The primary input for the model will be thermal images taken from the internal cameras at W7X.
Since the thermal camera data is sparse, with most of the pixels for any given shot containing zeros or noise, a convolutional neural network with that is optimized for sparse data would be ideal.

Convolutional neural networks use two dimensional convolutions applied to the inputs to generate new features (see Fig. \ref{fig:code:2DConv}).
These convolutions are stacked, being applied to the same input to produce multiple outputs which are concatenated along the z-axis.
Typically, strides (the number of pixels skipped over when translating the kernel) and padding (adding pixels around the edge of the image) allow convolutions to change the output x and y dimensions.
So as more layers of convolutions are successively applied, the outputs of each layer grow in z-height, and shrink in x and y.
The final output of the convolutional network is then passed to a fully connected network.


\begin{figure}[htb]
    \includegraphics[width=\textwidth]{images/2d-Conv.png}
    \caption{Example of a single 3x3 convolution operation. The source image is convolved with a 3x3 kernel. The kernel's weights are determined using backpropagation. [cite source]}
    \label{fig:code:2DConv}
\end{figure}

\label{sec:code:inceptnet}

The inceptnet architecture [cite https://arxiv.org/abs/1409.4842] has been demonstrated by Böckenhoff [insert ref to Daniel's paper] to be effective on a simplified version of this problem.
In initial tests it was was very promising for this application and met an important boundary conditions, mainly the network fit into the available video memory on the system used to train the network.
Inceptnet uses a series of sub-networks, known as inception modules (see fig. \ref{fig:code:inceptmodule}).
Each inception module is made up of 3 convolutional paths (1x1, 3x3, and 5x5) and a pooling path.
To assure the outputs from each path have the same resolution, padding is added to the 3x3 and 5x5 convolutions.
The number of each convolutional filter can be adjusted at each step and the outputs are concatenated together.
The idea behind having multiple convolution sizes is that some features of the input might be lost if only size of kernel is applied.

\begin{figure}[htb]
    \includegraphics[width=\textwidth]{images/incept-simple.png}
    \caption{A diagram of the inception module. The inputs from the prior layer are fed into 4 paths, applying 1x1, 3x3, 5x5, convolutions with the final layer being 3x3 max pooling. The outputs of each layer are concatenated and passed to the next incept module. The number of 1x1, 3x3, and 5x5 convolutions per incept module can be adjusted.}
    \label{fig:code:inceptmodule}
\end{figure}

Each of the paths in the four branches of the inception module play a different role.
The 1x1 filter doesn't take into account any patterns in the inputs height and width, but it does work across channels (depth) of the image.
This acts to reduce the dimension of the input while connecting the values from the other channels of the input.
Lin et. al in their 2014 paper [cite https://arxiv.org/pdf/1312.4400.pdf] referred to the 1x1 convolution as a "network in a network."
The 1x1 convolution can also be used to reduce the number of channels as the output of the filter will be 1x1x$n$, where $n$ is the number of 1x1 filters being applied.
This is why each branch has a 1x1 convolution.
The 1x1 convolutions marked in yellow in fig. \ref{fig:code:inceptmodule} are designed to reduce the number of channels input to the higher computational cost which dramatically reduces the number of weights needed for a module.

The 3x3 and 5x5 convolutions look at spacial patterns across the width, height, and depth of the input.
In this way they are actually a 3-dimensional convolution, but most convolutions are referred to by their 2 dimensional cross-section.
These two filters are typical optimized for finding features of the input at different scales.
Prior to this multi-branch approach, machine learning scientists would hand select the ordering of the various convolution sizes to be applied in succession.
By providing multiple paths and allowing the training process to determine which branches are necessary for a given step takes the human bias out of the training process.

There are several other structures that appear in the inception module.
One being a 3x3 max pooling layer, which is like a 3x3 convolution, but instead of taking the element-wise product with the filter, it returns the max pixel value over the region of the feature map that overlaps with the filter.
Pooling layers are a method of downsampling the input while attempting to preserve the most relevant pixels and are a staple of most deep convolutional networks.
The concatenation layer just concatenates the outputs from the various branches together.
Each branch has padding so that the height and width channels are aligned to make concatenation simple.

Lastly, there are batch normalization layers after every operation on each brach of the inception module.
Batch normalization is powerful tool in any deep learning model.
It has been show by S. Ioffe and C. Szegedy in their 2015 paper [cite https://arxiv.org/pdf/1502.03167.pdf] that batch normalization largest solves the issue of internal covariate shift, where layers in the network shift the input distribution to the next layer.
Since the inputs of any layer in the network are affected by the weights of every prior layer, small changes in the distribution get amplified as they propagate throughout the network.
To address this during training batches of data are normalized, transforming the neurons output using the first and second statistical moments (mean and standard deviation) across the batch.
Two additional trainable parameters, $\gamma$ and $\beta$ are applied after normalization to specify the output distribution, so each layer can achieve a unique output distribution based on the optimal one for the next operation or layer.

Incept modules also have two major advantages over classic deep convolutional networks, they require far fewer parameters, which reduced the overall memory footprint of the model and far fewer floating point operations than prior convolutional modules.
For example, using a 5x5 convolution to produce the same number of output channels as a incept module requires over 7 times the number of model parameters and floating point operations compared to the incept module.
However, despite having significantly fewer model parameters, when the inceptnet was release, it was the state of the art architecture.
Inceptnet outperformed models with the higher parameter count 5x5 and 3x3 modules.

The inception modules are stacked repeatedly (see fig. /ref{}), varying the number of filters in each brach of the incept modules in each layer of modules.




Initially, the labels for the data set were not produce, since the simulations necessary to produce them take a great deal of time, simplified versions of the labels which just took into account the ratio of the ratio of the [Check with Andrea].
We can see in Eq. [Insert ref to Iota eq] that the first order term is the magnetic field produce by the coils so this will act as a good approximation.
The inceptnet architecture had three promising advantages over the other options.
The model size was small enough to fit entirely in the v-Ram of the available GPUs, the initial testing of the ResNet showing good success with the proxy for iota with limited tuning of the network, and the prior success with the architecture by Böckenhoff.
Because of these reasons it was the network selected to be used.


\label{sec:code:philosophy}
The design philosophy of this project was important. The code was designed to be a complete package, to be easily used by fellow colleges, and to work with the state of the art tools available in the machine learning space.
I went with the PyTorch framework, as many of my colleges used this as well, and the PyTorch Lightning package provided a powerful way to design an object-oriented package.
For lower-level operations, like managing and recording the network configuration files, hyperparmeter searches, as well as structuring and analyzing results, the package Hydra, developed by Facebook, was used.
Git was used for the version control system and DVC for data version control.
This project was profiled and optimized with help from the PyTorch Lightning profiler.
Code comments and documentation have been developed to help with quick adoption of the code for future use.
Since future changes to the divertor surface will result in drastically different inputs to the network, the code was designed to automatically scale with the inputs.
The design philosophy was focused on making a project that could be quickly and easily taken over by a future student and be powerful and flexible enough to work with expected changes in the future.



\label{sec:code:hyperparameters}

Once the broad architecture was selected the next step is to tune the model.
Tuning the model helps model level parameters, like the number of filters at a given module, to be optimized for the use case.
Tuning was done systematically with Optuna and results were recorded and compared using Tensorboard and MLFlow.
You can see from the table [Insert graph] the results of tuning the hyperparameter for the optimization step size $\alpha$.




% \section{Postcards: My Address}
% \label{sec:intro:address}

% \textbf{Ricardo Langner} \\
% Alfred-Schrapel-Str. 7 \\
% 01307 Dresden \\
% Germany



% \section{Motivation and Problem Statement}
% \label{sec:intro:motivation}

% \Blindtext[3][1] \cite{Jurgens:2000,Jurgens:1995,Miede:2011,Kohm:2011,Apple:keynote:2010,Apple:numbers:2010,Apple:pages:2010}

% \section{Results}
% \label{sec:intro:results}

% \Blindtext[1][2]

% \subsection{Some References}
% \label{sec:intro:results:refs}

% \cite{WEB:GNU:GPL:2010,WEB:Miede:2011}
% \Blindtext[1][1]

% \subsubsection{Methodology}
% \label{sec:intro:results:refs:method}

% \Blindtext[1][2]

% \paragraph{Strategy 1}
% \Blindtext[1][1]

% \begin{lstlisting}[language=Python, caption={This simple helloworld.py file prints Hello World.}\label{lst:pyhelloworld}]
% #!/usr/bin/env python
% print "Hello World"
% \end{lstlisting}

% \paragraph{Strategy 2}
% \Blindtext[1][1]

% \begin{lstlisting}[language=Python, caption={This is a bubble sort function.}\label{lst:pybubblesort}]
% #!/usr/bin/env python
% def bubble_sort(list):
%     for num in range(len(list)-1,0,-1):
%         for i in range(num):
%             if list[i]>list[i+1]:
%                 tmp = list[i]
%                 list[i] = list[i+1]
%                 list[i+1] = tmp

% alist = [34,67,2,4,65,16,17,95,20,31]
% bubble_sort(list)
% print(list)
% \end{lstlisting}

% \section{Thesis Structure}
% \label{sec:intro:structure}

% \textbf{Chapter \ref{sec:related}} \\[0.2em]
% \blindtext

% \textbf{Chapter \ref{sec:system}} \\[0.2em]
% \blindtext

% \textbf{Chapter \ref{sec:concepts}} \\[0.2em]
% \blindtext

% \textbf{Chapter \ref{sec:concepts}} \\[0.2em]
% \blindtext

% \textbf{Chapter \ref{sec:conclusion}} \\[0.2em]
% \blindtext
